# 7회차 LLM 과제 보고서

## Prompting Method별 정답률 비교

| Prompting Method | 0 shot | 3 shot | 5 shot |
| :--------------: | :----: | :----: | :----: |
| Direct Prompting |  0.18  |  0.20  |  0.18  |
|  CoT Prompting   |  0.60  |  0.64  |  0.72  |
|   My Prompting   |  0.72  |  0.66  |  0.74  |

## CoT Prompting이 Direct Prompting에 비해 가질 수 있는 강점!

**Chain-of-Thought (CoT)** 방식은 정답만 요구하는 Direct Prompting보다 다음과 같은 점에서 강점을 가집니다.

- **문제 해결 과정 유도**: step-by-step 방식으로 중간 추론 단계를 명시해 오류를 줄이고, 복잡한 문제에서도 안정적으로 정답을 도출할 수 있습니다.
- **LLM의 언어 기반 추론 능력 활용**: 단순 계산보다는 설명을 중심으로 한 문제 해결이 모델에 더 잘 맞습니다.
- **후속 전략과 결합 용이**: reasoning이 포함된 응답은 self-consistency prompting과 같은 고급 기법과의 연계도 자연스럽습니다.

## My Prompting이 CoT보다 더 효과적인 이유

저는 단순히 step-by-step 지시만 주는 CoT를 넘어, **모델의 역할, 표현 형식, 예시 품질**까지 설계에 반영했습니다.

### 1. 도메인 맞춤 역할 지시
- 모델에게 “**세계적인 초등학교 수학 튜터**” 역할을 부여해, GSM8K 수준에 맞는 친절하고 명확한 설명을 유도했습니다.

### 2. 형식의 일관성 확보
- 모든 계산을 `<< >>`로 감싸고, 정답은 `'#### <final answer>'` 형식으로 출력하게 했습니다.
- 이로 인해 파싱이 쉬워지고, 모델이 일관된 출력을 학습하도록 도왔습니다.  

### 3. 고품질 예시만 활용
- reasoning이 잘 드러나는 예시만 필터링해 사용했습니다 (e.g. 길이, 줄바꿈 수, 표기 기준 등).
- 덕분에 모델이 혼동 없이 좋은 패턴만 학습할 수 있었습니다.

### 4. 자기 점검 유도
- 지시문에 "Think carefully and double-check each step."을 추가해, 모델이 스스로 점검하게 만들었습니다.
- 이는 단순 정답 도출을 넘어서 추론의 신뢰도까지 높이는 요소였습니다.

## 결론

단순히 reasoning을 하게끔 유도하는 수준을 넘어서, **문제 유형에 맞는 역할 설정과 형식적 통일성, 정제된 예시, 자기 점검 유도**까지 종합적으로 설계한 것이 CoT보다 더 높은 정답률로 이어졌다고 생각합니다. LLM의 능력을 실제 문제 해결에 효과적으로 활용하려면, 이처럼 다양한 관점에서 prompt를 고민하는 것이 중요하다는 것을 이번 과제를 통해 배울 수 있었습니다.